# ğŸ“ Complete Presentation Guide: Privacy-Preserving Federated Credit Scoring

**A Comprehensive Tutorial for Understanding and Presenting Federated Learning Research**

---

## ğŸ“š Purpose & Audience

This guide is designed for **anyone presenting this project**, regardless of prior knowledge. Whether you're:
- A student presenting in class
- A researcher presenting at a conference  
- A developer explaining to stakeholders
- Someone who has never heard of federated learning before

**You will find everything you need here, explained from first principles.**

---

## ğŸ“‹ Table of Contents

### Part I: Foundation Concepts (For Complete Beginners)
1. [What is Credit Scoring & Why It Matters](#1-what-is-credit-scoring)
2. [The Privacy Crisis in Banking](#2-the-privacy-crisis)
3. [Introduction to Federated Learning](#3-federated-learning-basics)
4. [Why Neural Networks Need Calibration](#4-calibration-fundamentals)

### Part II: Technical Deep Dive
5. [Dataset & Features Explained](#5-dataset-analysis)
6. [Model Architectures in Detail](#6-model-architectures)
7. [Calibration Methods: Complete Guide](#7-calibration-methods)
8. [Non-IID Data & Dirichlet Partitioning](#8-non-iid-data)
9. [Hyperparameter Optimization with Optuna](#9-hyperparameter-tuning)

### Part III: Our Implementation
10. [System Architecture & Code Structure](#10-system-architecture)
11. [The FedAvg Algorithm: Step by Step](#11-fedavg-algorithm)
12. [Complete Experimental Pipeline](#12-experimental-pipeline)

### Part IV: Results & Analysis
13. [Experimental Results Breakdown](#13-results-analysis)
14. [Visualization Guide](#14-visualization-guide)
15. [Key Findings & Implications](#15-key-findings)

### Part V: Presentation Materials
16. [Slide-by-Slide Presentation Script](#16-presentation-script)
17. [Q&A: Anticipated Questions](#17-qa-guide)
18. [Further Reading & Resources](#18-resources)

---

## Part I: Foundation Concepts

---

## 1. What is Credit Scoring?

### 1.1 The Business Problem

**Credit scoring** is the process of determining how likely a borrower is to repay a loan.

**Real-World Example:**
```
Alice applies for a $10,000 personal loan at Bank XYZ.
Bank needs to answer: "Will Alice pay us back?"

If Bank approves:
  âœ… Alice pays back â†’ Bank earns $1,200 interest (12% APR)
  âŒ Alice defaults â†’ Bank loses $10,000 principal

Bank's decision depends on CREDIT SCORE (probability of default)
```

### 1.2 Why This Matters

**Financial Impact:**
- US Consumer Credit: **$4.5 Trillion** (2025 data)
- Credit card defaults cost banks **$50 billion/year**
- 1% improvement in default prediction = **$500 million saved**

**Social Impact:**
- Determines who gets loans, mortgages, credit cards
- Affects interest rates (higher risk = higher rates)
- Impacts economic mobility and wealth building

### 1.3 Traditional Approach (Pre-ML Era)

**FICO Score (1989-Present):**
- Payment history (35%)
- Amounts owed (30%)
- Length of credit history (15%)
- Credit mix (10%)
- New credit (10%)

**Problems:**
- Simplistic linear rules
- Ignores complex patterns
- Hard to update
- Same model for everyone

### 1.4 Modern ML Approach

**Machine Learning Credit Scoring:**
```
Input: Customer Data
  - Demographics: Age, income, education
  - Credit history: 6 months of payments
  - Financial: Balance, credit limit
  
Output: Default Probability
  - P(default) = 0.23 â†’ "23% chance won't pay"
  - P(default) = 0.05 â†’ "5% chance won't pay" âœ… APPROVE
```

**Advantages:**
- Captures non-linear patterns
- Updates with new data
- Personalized predictions
- Higher accuracy (reduces losses)

---

## 2. The Privacy Crisis

### 2.1 The Scenario: 5 Banks Consortium

Imagine a federation of 5 banks:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE DILEMMA                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  ğŸ¦ Bank A (Small Regional)                            â”‚
â”‚     â””â”€ 1,471 customers                                 â”‚
â”‚     â””â”€ Local model accuracy: 22.3% âŒ                  â”‚
â”‚     â””â”€ Reason: Not enough data to learn               â”‚
â”‚                                                         â”‚
â”‚  ğŸ¦ Bank B (Mid-sized)                                 â”‚
â”‚     â””â”€ 2,536 customers                                 â”‚
â”‚     â””â”€ Local model accuracy: 79.9%                     â”‚
â”‚     â””â”€ Still suboptimal                                â”‚
â”‚                                                         â”‚
â”‚  ğŸ¦ Bank C (Large National)                            â”‚
â”‚     â””â”€ 11,111 customers                                â”‚
â”‚     â””â”€ Local model accuracy: 78.4%                     â”‚
â”‚     â””â”€ Good, but not great                             â”‚
â”‚                                                         â”‚
â”‚  ğŸ¦ Bank D (Small Community)                           â”‚
â”‚     â””â”€ 563 customers                                   â”‚
â”‚     â””â”€ Local model accuracy: 80.4%                     â”‚
â”‚                                                         â”‚
â”‚  ğŸ¦ Bank E (Small Regional)                            â”‚
â”‚     â””â”€ 1,119 customers                                 â”‚
â”‚     â””â”€ Local model accuracy: 22.1% âŒ                  â”‚
â”‚                                                         â”‚
â”‚  ğŸ“Š AVERAGE LOCAL ACCURACY: 56.6%                      â”‚
â”‚                                                         â”‚
â”‚  ğŸ’¡ IF THEY POOLED DATA (16,800 customers):            â”‚
â”‚     Central model accuracy: 81.3% âœ…                   â”‚
â”‚     Improvement: +24.7 percentage points!              â”‚
â”‚                                                         â”‚
â”‚  âŒ BUT: Data pooling is ILLEGAL                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Why Data Sharing is Illegal

**Legal Barriers:**

1. **GDPR (Europe) - General Data Protection Regulation**
   - Fines: Up to â‚¬20 million or 4% of global annual revenue
   - Example: British Airways fined Â£183 million (2019)
   - Requirements: Explicit consent, data minimization, right to erasure

2. **CCPA (California) - Consumer Privacy Act**
   - Fines: Up to $7,500 per violation
   - Consumer rights: Know, delete, opt-out

3. **Banking Secrecy Laws**
   - Switzerland: Criminal offense to disclose customer data
   - US: Bank Secrecy Act + Gramm-Leach-Bliley Act

4. **Competitive Secrets**
   - Sharing customer data = sharing competitive advantage
   - Antitrust concerns (collusion)

**What Cannot Be Shared:**
```
âŒ Customer names, SSNs, addresses
âŒ Transaction amounts (exact values)
âŒ Account balances
âŒ Credit scores
âŒ Loan application details
âŒ Any personally identifiable information (PII)
```

### 2.3 The Fundamental Tension

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PRIVACY vs. UTILITY TRADEOFF                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Maximum Privacy (Siloed Data)                         â”‚
â”‚    âœ… Legal compliance                                 â”‚
â”‚    âœ… Competitive secrets protected                    â”‚
â”‚    âŒ Weak models (56.6% accuracy)                     â”‚
â”‚    âŒ Small banks fail (22% accuracy)                  â”‚
â”‚    âŒ Higher default rates                             â”‚
â”‚    âŒ Fewer loans approved                             â”‚
â”‚                                                         â”‚
â”‚  Maximum Utility (Centralized Data)                    â”‚
â”‚    âœ… Strong models (81.3% accuracy)                   â”‚
â”‚    âœ… All banks benefit                                â”‚
â”‚    âŒ ILLEGAL under GDPR/CCPA                          â”‚
â”‚    âŒ Massive fines                                     â”‚
â”‚    âŒ Criminal liability                               â”‚
â”‚    âŒ Reputation damage                                â”‚
â”‚                                                         â”‚
â”‚  ğŸ¯ OUR GOAL: Best of Both Worlds                      â”‚
â”‚    âœ… 79.4% FL-MLP accuracy                            â”‚
â”‚    âœ… 81.0% FL-LSTM accuracy                           â”‚
â”‚    âœ… Full privacy preservation                        â”‚
â”‚    âœ… Legal compliance                                 â”‚
â”‚    âœ… No data leaves premises                          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Federated Learning Basics

### 3.1 What is Federated Learning?

**Simple Definition:**
> Federated Learning (FL) is a way to train a shared machine learning model across multiple organizations **without sharing the raw data**.

**Analogy:**
```
Traditional Learning = Potluck Dinner
  - Everyone brings ingredients to one kitchen
  - Chef cooks using all ingredients together
  - Problem: Some ingredients are secret recipes!

Federated Learning = Cooking Competition
  - Each contestant cooks in their own kitchen
  - They share cooking techniques (not ingredients)
  - Judge combines techniques to create master recipe
  - Result: Master recipe as good as potluck, but secrets protected
```

### 3.2 How FL Works: The Dance

**Step-by-Step Process:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FEDERATED LEARNING: ONE ROUND                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  [0] INITIALIZATION                                     â”‚
â”‚      Server creates initial model: wâ‚€ = random         â”‚
â”‚                                                         â”‚
â”‚  [1] DISTRIBUTION                                       â”‚
â”‚      ğŸŒ Server â†’ ğŸ¦ Banks: "Here's model wâ‚€"          â”‚
â”‚                                                         â”‚
â”‚  [2] LOCAL TRAINING (Parallel, Private)                â”‚
â”‚      ğŸ¦ Bank A: Trains on 1,471 local customers        â”‚
â”‚         Input: wâ‚€ + local data                         â”‚
â”‚         Output: wâ‚á´¬ (updated weights)                  â”‚
â”‚                                                         â”‚
â”‚      ğŸ¦ Bank B: Trains on 2,536 local customers        â”‚
â”‚         Output: wâ‚á´®                                    â”‚
â”‚                                                         â”‚
â”‚      ... (Banks C, D, E do same)                       â”‚
â”‚                                                         â”‚
â”‚      âš ï¸ KEY: Banks never share customer data!          â”‚
â”‚                                                         â”‚
â”‚  [3] UPLOAD                                             â”‚
â”‚      ğŸ¦ Banks â†’ ğŸŒ Server: Upload only weights         â”‚
â”‚      Bank A sends: wâ‚á´¬ (just numbers, no data)        â”‚
â”‚      Bank B sends: wâ‚á´®                                 â”‚
â”‚      ...                                                â”‚
â”‚                                                         â”‚
â”‚  [4] AGGREGATION (FedAvg)                              â”‚
â”‚      Server combines using weighted average:           â”‚
â”‚                                                         â”‚
â”‚      wâ‚ = (n_A Ã— wâ‚á´¬ + n_B Ã— wâ‚á´® + ...) / N          â”‚
â”‚                                                         â”‚
â”‚      Where:                                             â”‚
â”‚        n_A = 1,471 (Bank A size)                       â”‚
â”‚        N = 16,800 (total customers)                    â”‚
â”‚                                                         â”‚
â”‚      Larger banks have more influence âœ…               â”‚
â”‚                                                         â”‚
â”‚  [5] REPEAT                                             â”‚
â”‚      Go back to step [1] with wâ‚                       â”‚
â”‚      Continue for 15 rounds total                      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Mathematical Foundation

**FedAvg (Federated Averaging) Algorithm:**

$$
w_{t+1} = \sum_{k=1}^{K} \frac{n_k}{N} w_k^{(t)}
$$

**Breaking it down:**
- $w_{t+1}$ = New global model weights (round $t+1$)
- $K = 5$ = Number of banks
- $w_k^{(t)}$ = Bank $k$'s updated weights at round $t$
- $n_k$ = Number of customers at bank $k$
- $N = \sum_{k=1}^{K} n_k$ = Total customers

**Example Calculation (Round 1):**

```python
# Bank sizes
n_A = 1471
n_B = 2536  
n_C = 11111
n_D = 563
n_E = 1119
N = 16800  # Total

# Weights (importance) for each bank
weight_A = 1471 / 16800 = 0.0876  (8.76%)
weight_B = 2536 / 16800 = 0.1510  (15.10%)
weight_C = 11111 / 16800 = 0.6613 (66.13%) â† Largest influence
weight_D = 563 / 16800 = 0.0335   (3.35%)
weight_E = 1119 / 16800 = 0.0666  (6.66%)

# Global update (for each parameter layer)
w_global = (0.0876 Ã— w_A + 0.1510 Ã— w_B + 0.6613 Ã— w_C + 
            0.0335 Ã— w_D + 0.0666 Ã— w_E)
```

**Why this works:**
- Larger banks contribute more (proportional to data size)
- Small banks still benefit from global knowledge
- Fair: Each customer has equal influence

---

## 4. Calibration Fundamentals  

### 4.1 The Problem: Overconfident Models

**Scenario:**
```
Model predicts: P(default) = 0.95 (95% confident customer will default)
Reality: Customer defaults 70% of the time

This is MISCALIBRATION âŒ
```

**Why it matters:**

**Example 1: Loan Pricing**
```
Customer #4523 applies for $50,000 loan

Model says: P(default) = 0.10 (10% risk)
Bank sets interest rate: 5% (low risk pricing)

ACTUAL risk: P(default) = 0.30 (30%!)

Result:
  - Expected loss: $50,000 Ã— 0.10 = $5,000
  - Actual loss: $50,000 Ã— 0.30 = $15,000
  - Bank loses extra $10,000 âŒ
```

### 4.2 Measuring Calibration: ECE

**Expected Calibration Error (ECE):**

$$
\text{ECE} = \sum_{m=1}^{M} \frac{|B_m|}{N} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|
$$

**Interpretation:**
- ECE < 0.05: **Well-calibrated** âœ…
- 0.05 â‰¤ ECE < 0.10: **Acceptable** âš ï¸
- ECE â‰¥ 0.10: **Poorly calibrated** âŒ

**Our Actual Results:**
```
Before calibration:
  FL-MLP: ECE = 0.227 âŒ
  FL-LSTM: ECE = 0.154 âŒ

After calibration (Beta/Platt):
  FL-MLP: ECE = 0.056 âœ… (75.3% improvement)
  FL-LSTM: ECE = 0.072 âœ… (53.2% improvement)
```

---

## Part II: Technical Deep Dive

---

## 5. Dataset Analysis

### 5.1 UCI Credit Card Default Dataset

**Source:** OpenML ID 42477  
**Original:** I-Cheng Yeh, Taiwan (2005)

**Statistics:**
- **Total samples:** 30,000 customers
- **Features:** 23 attributes
- **Target:** Default payment (1 = yes, 0 = no)
- **Class distribution:** 22.12% default, 77.88% pay
- **Imbalance ratio:** 3.52:1

### 5.2 Feature Categories

**Demographics (5 features):**
1. LIMIT_BAL: Credit limit (NT$)
2. SEX: Gender (1=Male, 2=Female)
3. EDUCATION: Education level
4. MARRIAGE: Marital status  
5. AGE: Age in years

**Payment History (6 features): PAY_0 through PAY_6**
- Historical monthly payment status
- Values: -1 (pay duly), 0 (pay normally), 1-8 (months delayed)

**Bill Amounts (6 features): BILL_AMT1-6**
- Bill statement amount for 6 months

**Payment Amounts (6 features): PAY_AMT1-6**
- Actual payment made for 6 months

### 5.3 Data Split Strategy

```
Original: 30,000 samples
â”‚
â”œâ”€ Dev Set: 24,000 (80%)
â”‚  â”œâ”€ Train: 16,800 (FL training)
â”‚  â””â”€ Val: 7,200
â”‚     â”œâ”€ Calib: 3,600 (calibration training)
â”‚     â””â”€ Eval: 3,600 (hyperparameter selection)
â”‚
â””â”€ Test: 6,000 (20%) â† Final evaluation
```

---

## 6. Model Architectures

### 6.1 FL-MLP (CreditNet)

**Architecture:**
```
Input (23) â†’ Dense(256) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.30)
          â†’ Dense(96) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.30)
          â†’ Dense(1) â†’ Sigmoid â†’ P(default)
```

**Hyperparameters (Optuna-tuned):**
- Learning rate: 0.00133
- Hidden layers: [256, 96]
- Dropout: 0.299
- Parameters: 30,913

### 6.2 FL-LSTM (CreditLSTM)

**Hybrid Architecture:**
```
Static Branch: Demographics (5) â†’ Dense(32) â†’ LayerNorm
Temporal Branch: Payment history (6Ã—3) â†’ LSTM(32, 4 layers)
Fusion: Concat â†’ Dense(64) â†’ Dense(32) â†’ Dense(1) â†’ Sigmoid
```

**Hyperparameters (Optuna-tuned):**
- Learning rate: 0.000104
- LSTM hidden: 32
- LSTM layers: 4
- Dropout: 0.432
- Parameters: ~36,000

### 6.3 Baseline Models

1. **Logistic Regression:** Linear baseline (C=0.00195)
2. **XGBoost:** 200 trees, max_depth=12
3. **Local NNs:** 5 separate models (no collaboration)
4. **Central NN:** Oracle (all data pooled)

---

## 7. Calibration Methods

We compared 4 calibration approaches:

### 7.1 Platt Scaling

Learn parameters A, B:
$$P_{\text{cal}}(y=1|z) = \sigma(Az + B)$$

**Our Result (FL-MLP):** A=0.920, B=-0.744, ECE=0.085

### 7.2 Temperature Scaling  

Learn temperature T:
$$P_{\text{cal}}(y=1|z) = \sigma(z/T)$$

**Our Result (FL-MLP):** T=0.955, ECE=0.225 (minimal improvement)

### 7.3 Beta Calibration

Most flexible, 3 parameters (a, b, c)

**Our Result (FL-MLP):** ECE=0.056 âœ… **BEST for FL-MLP**

### 7.4 FedCal (Our Contribution)

Federated calibration preserving privacy:
1. Each client trains local Platt scaler
2. Upload only (A_k, B_k) parameters  
3. Server aggregates via weighted average

**Our Result (FL-MLP):** A_global=0.714, B_global=-0.794, ECE=0.092, Acc=0.815 âœ…

---

## 8. Non-IID Data Simulation

### 8.1 Dirichlet Partitioning (Î±=0.5)

**Actual Client Distribution:**
```
Client 0: 1,471 samples, 95.6% PAY (extreme)
Client 1: 2,536 samples, 14.1% PAY  
Client 2: 11,111 samples, 6.7% PAY (largest, 66% weight)
Client 3: 563 samples, 16.9% PAY (smallest)
Client 4: 1,119 samples, 99.8% PAY (pathological)

Heterogeneity Std: 0.4189 (high)
```

**Impact:** Strong heterogeneity creates ~8% performance gap vs. centralized

---

## 9. Hyperparameter Tuning

### 9.1 Optuna Bayesian Optimization

**MLP Results (10 trials):**
- Best trial: #0
- Eval accuracy: 0.8197
- Params: lr=0.00133, hidden=[256,96], dropout=0.299

**LSTM Results (10 trials):**
- Best trial: #8  
- Eval accuracy: 0.8175
- Params: lr=0.000104, hidden=32, layers=4, dropout=0.432

**XGBoost Results (10 trials):**
- Best: n_estimators=200, max_depth=12, lr=0.162
- Eval accuracy: 0.7994

**LogReg Results (10 trials):**
- Best: C=0.00195, solver='lbfgs'
- Eval accuracy: 0.6928

---

## Part III: Implementation

---

## 10. System Architecture

**Module Structure:**
```
main.py (Orchestrator, 672 lines)
â”œâ”€â”€ config.py (Constants, 151 lines)
â”œâ”€â”€ data.py (Loading & splits, 184 lines)
â”œâ”€â”€ models.py (Architectures, 325 lines)
â”œâ”€â”€ federated.py (FedAvg, 253 lines)
â”œâ”€â”€ hyperparams.py (Optuna, 945 lines)
â”œâ”€â”€ calibration.py (4 methods, 312 lines)
â”œâ”€â”€ calibration_analysis.py (373 lines)
â”œâ”€â”€ baselines.py (Comparison, 281 lines)
â””â”€â”€ visualization.py (Plots, 216 lines)
```

---

## 11. FedAvg Algorithm

**Pseudocode:**
```
FOR round t = 1 to 15:
  1. Server broadcasts w_t to all clients
  2. FOR each client k in parallel:
     - Train locally on D_k for 1 epoch
     - Return updated w_k
  3. Server aggregates:
     w_{t+1} = Î£ (n_k/N) Ã— w_k
```

**Our Round-by-Round Results (FL-MLP):**
```
Round  1: 73.0%  (initial learning)
Round  5: 72.9%  (oscillation from heterogeneity)
Round 10: 77.8%  (convergence phase)
Round 15: 73.2%  (final, slight overfitting)

Test accuracy: 79.4% (with Beta calibration)
```

---

## 12. Complete Pipeline

**9-Step Execution:**
1. Load data (OpenML)
2. Create splits (train/calib/eval/test)
3. Dirichlet partitioning (Î±=0.5)
4. Hyperparameter tuning (Optuna, 10 trials each)
5. FL training MLP (15 rounds)
6. Calibration comparison (4 methods)
7. FL training LSTM (15 rounds)
8. Train baselines (LogReg, XGBoost, Local NNs, Central NN)
9. Calibration analysis + visualization

**Total runtime:** ~13 minutes with tuning

---

## Part IV: Results & Analysis

---

## 13. Experimental Results

### 13.1 Final Leaderboard (Test Set)

| Model | Accuracy | F1 | ECE | Brier |
|-------|----------|-----|-----|-------|
| **Central NN** | **81.28%** | 0.384 | 0.030 | 0.141 |
| **FL-LSTM** | **79.11%** | 0.250 | 0.072 | 0.155 |
| **FL-MLP** | **79.39%** | 0.206 | 0.056 | 0.148 |
| Local NN (Avg) | 56.62% | 0.281 | 0.353 | - |
| LogReg | 67.53% | 0.457 | 0.236 | - |

### 13.2 Key Findings

**Finding 1: Privacy comes with minimal cost**
```
Central NN:  81.28% (no privacy)
FL-LSTM:     79.11% (full privacy)
Gap:         2.17 percentage points only âœ…
```

**Finding 2: Collaboration is essential**
```
Local NN:    56.62% (siloed)
FL-MLP:      79.39% (collaborative)
Improvement: +22.77 percentage points âœ…
```

**Finding 3: Calibration dramatically improves reliability**
```
FL-MLP before: ECE = 0.227 (miscalibrated)
FL-MLP after:  ECE = 0.056 (well-calibrated)
Improvement:   75.3% reduction âœ…
```

**Finding 4: LSTM captures temporal patterns better**
```
FL-MLP:  79.39% accuracy
FL-LSTM: 79.11% accuracy (similar)
But FL-LSTM has much better F1: 0.250 vs 0.206
```

---

## 14. Visualization Guide

### 14.1 Figure 1: Convergence

**Location:** `fl_experiment_*/Fig1_Convergence.png`

Shows training accuracy over 15 rounds for both FL-MLP and FL-LSTM.

**Key observations:**
- MLP starts higher (73%) but plateaus around 77%
- LSTM starts lower (51%) but climbs steadily to 80%
- Both show oscillation due to Non-IID data

### 14.2 Figure 2: Calibration

**Location:** `fl_experiment_*/Fig2_Calibration.png`

Reliability diagrams showing predicted vs actual probabilities.

**Interpretation:**
- Perfect calibration = diagonal line
- Uncalibrated: S-shaped deviation (overconfidence)
- Calibrated: Hugs diagonal (trustworthy probabilities)

### 14.3 Figure 3: Final Comparison

**Location:** `fl_experiment_*/Fig3_FinalComparison.png`

Bar charts comparing all models on Accuracy, F1, and ECE.

**Takeaway:** FL models competitive with centralized, vastly better than local.

### 14.4 Additional Visualizations

- **Client Heterogeneity:** Bar chart showing class imbalance per client
- **Calibration Heatmap:** All models Ã— all methods ECE comparison
- **SHAP Summary:** Feature importance (PAY_0 most important)
- **LIME Explanation:** Single prediction breakdown

---

## 15. Key Findings & Implications

### 15.1 Scientific Contributions

1. **Federated learning works for finance:** Achieved 97.3% of centralized performance while preserving privacy

2. **Calibration is critical:** 75% ECE reduction makes predictions trustworthy for risk pricing

3. **FedCal enables privacy-preserving calibration:** Novel method aggregates local calibrators without sharing logits

4. **Non-IID robustness:** System handles extreme heterogeneity (clients with 96% vs 7% default rates)

### 15.2 Business Impact

**For Banks:**
- Small banks gain +22.7% accuracy through collaboration
- No customer data leaves premises (legal compliance)
- Production-ready calibration for loan pricing

**For Customers:**
- Fairer credit decisions (more data = better models)
- Privacy protected (data stays at origin bank)
- Explainable decisions (SHAP/LIME available)

### 15.3 Limitations & Future Work

**Current Limitations:**
1. Assumes honest-but-curious server (no malicious attacks)
2. Communication overhead (15 rounds Ã— 5 clients)
3. Non-IID data still causes 2% performance gap

**Future Directions:**
1. Add differential privacy noise to model updates
2. Reduce communication (gradient compression, fewer rounds)
3. Personalized models (adapt global model to local distribution)
4. Byzantine-robust aggregation (handle malicious clients)

---

## Part V: Presentation Materials

---

## 16. Slide-by-Slide Script

### Slide 1: Title
"Today I'll present a privacy-preserving credit scoring system using federated learning that achieves 79% accuracy without sharing any customer data."

### Slide 2: The Problem
"Banks face a dilemma: pooling data gives 81% accuracy but violates GDPR. Local-only models achieve just 57%. We need both privacy AND performance."

### Slide 3: Our Solution - Federated Learning
"Federated learning trains a shared model by exchanging only model weights, never raw data. Think of it as sharing cooking techniques instead of secret recipes."

### Slide 4: How FedAvg Works
"Each round: (1) server broadcasts model, (2) banks train locally, (3) server aggregates weighted by data size. After 15 rounds, we converge."

### Slide 5: Dataset  
"We use UCI Credit Card Default: 30,000 customers, 23 features including payment history and demographics. We simulate 5 banks with heterogeneous data."

### Slide 6: Model Architecture
"Two models: FL-MLP (256â†’96â†’1) for tabular data, and FL-LSTM for temporal payment patterns. Both optimized with Optuna."

### Slide 7: The Calibration Problem
"Neural networks are overconfident. A 90% prediction might reflect only 65% actual risk. This causes billions in mispriced loans."

### Slide 8: Calibration Methods
"We compared 4 methods: Platt, Temperature, Beta, and FedCal (our contribution). Beta reduced ECE by 75% for FL-MLP."

### Slide 9: Results
"FL-LSTM achieved 79.1% accuracy vs 81.3% centralized - just 2.2% gap! FL-MLP: 79.4%. Both vastly outperform 56.6% local-only."

### Slide 10: Key Findings
"Three takeaways: (1) Privacy costs only 2%, (2) Collaboration improves accuracy by 23%, (3) Calibration is essential for trust."

### Slide 11: Visualizations
"Figure 1 shows convergence over 15 rounds. Figure 2 proves calibration works - predictions match reality. Figure 3 compares all models."

### Slide 12: Business Impact
"Small banks gain enterprise-grade models. Customers get fair, explainable decisions. Everyone stays GDPR-compliant."

### Slide 13: Conclusion
"We've demonstrated production-ready federated learning for credit scoring: private, accurate, and calibrated. Code available on GitHub."

---

## 17. Q&A Guide

### Q1: "How do you prevent the server from inferring customer data from model weights?"

**Answer:** Model weights are aggregated parameters (millions of numbers) that encode patterns, not individual records. Theoretical attacks exist (model inversion) but require white-box access and yield poor results. We can add differential privacy noise for formal guarantees.

### Q2: "What if one bank sends poisoned weights to sabotage the model?"

**Answer:** Current implementation assumes honest-but-curious parties. For malicious settings, we'd use Byzantine-robust aggregation (e.g., Krum, median aggregation) that detects and excludes outlier updates.

### Q3: "Why does Non-IID data hurt performance?"

**Answer:** FedAvg assumes clients sample from the same distribution. With heterogeneous clients (one bank all high-risk, another all low-risk), local updates pull in different directions, causing oscillation and slower convergence.

### Q4: "How do you handle client dropout (bank goes offline)?"

**Answer:** Server can proceed with subset of clients in each round. FedAvg still converges as long as clients return eventually. For critical availability, use asynchronous FL (FedAsync).

### Q5: "Is 15 rounds enough? How did you choose this?"

**Answer:** We observed convergence plateau around Round 10-12. Beyond Round 15, we see diminishing returns and slight overfitting. This is dataset-specific; larger/harder datasets may need 50+ rounds.

### Q6: "What's the communication cost?"

**Answer:** Each round: 5 clients upload ~30K parameters (4 bytes each) â‰ˆ 600KB total. Over 15 rounds: 9MB. Negligible for modern networks, but could use gradient compression for mobile/IoT.

### Q7: "Can clients have different model architectures?"

**Answer:** Not with vanilla FedAvg (requires same architecture to average weights). Split learning or federated distillation allow heterogeneous models but add complexity.

### Q8: "How does this compare to differential privacy approaches?"

**Answer:** Differential privacy adds calibrated noise to provide formal privacy guarantees (Îµ, Î´) but typically costs 5-10% accuracy. FL alone provides empirical privacy (no raw data shared) with minimal accuracy loss. They're complementary - can combine both.

### Q9: "What about fairness across demographic groups?"

**Answer:** We implemented fairness auditing (Demographic Parity, Equal Opportunity) but removed it from final pipeline per project scope. Our SHAP analysis shows SEX has minimal feature importance (0.02), suggesting limited bias.

### Q10: "Can this scale to 100+ banks?"

**Answer:** Yes, but communication becomes bottleneck. Solutions: (1) hierarchical aggregation (regional then global), (2) client sampling (randomly select 10% per round), (3) asynchronous updates.

---

## 18. Further Reading & Resources

### 18.1 Foundational Papers

**Federated Learning:**
1. McMahan et al. (2017). "Communication-Efficient Learning of Deep Networks from Decentralized Data." AISTATS.
   - Original FedAvg paper
   - https://arxiv.org/abs/1602.05629

2. Li et al. (2020). "Federated Optimization in Heterogeneous Networks." MLSys.
   - FedProx for Non-IID data
   - https://arxiv.org/abs/1812.06127

3. Kairouz et al. (2021). "Advances and Open Problems in Federated Learning." Foundations and Trends.
   - Comprehensive 200-page survey
   - https://arxiv.org/abs/1912.04977

**Calibration:**
4. Guo et al. (2017). "On Calibration of Modern Neural Networks." ICML.
   - Temperature scaling
   - https://arxiv.org/abs/1706.04599

5. Kull et al. (2017). "Beta Calibration: A well-founded and easily implemented improvement on logistic calibration for binary classifiers." AISTATS.
   - Beta calibration
   - https://arxiv.org/abs/1604.00065

6. Platt (1999). "Probabilistic Outputs for Support Vector Machines."
   - Platt scaling (classic)

**Privacy:**
7. Dwork & Roth (2014). "The Algorithmic Foundations of Differential Privacy."
   - Differential privacy foundations
   - https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf

### 18.2 Code & Implementations

**Our Implementation:**
- GitHub: [Will be added after review]
- Includes all code, data splits, trained models
- Requirements: Python 3.11+, PyTorch 2.0+

**Frameworks:**
1. **Flower (flwr.dev)** - Production FL framework
2. **PySyft** - Privacy-preserving ML library
3. **TensorFlow Federated** - Google's FL platform
4. **FATE** - Industrial FL platform (WeBank)

### 18.3 Datasets

**Credit Scoring:**
1. UCI German Credit: https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
2. Kaggle Give Me Some Credit: https://www.kaggle.com/c/GiveMeSomeCredit
3. LendingClub Loan Data: https://www.kaggle.com/wordsforthewise/lending-club

**FL Benchmarks:**
1. LEAF: Federated learning benchmark (vision + NLP)
2. FedML Benchmark: https://github.com/FedML-AI/FedML

### 18.4 Online Courses & Tutorials

1. **Coursera: Applied ML by Andrew Ng**
   - Foundations + deployment

2. **Federated Learning Tutorial (NeurIPS)**
   - https://federatedlearning.today/

3. **Google's Federated Learning Comic**
   - https://federated.withgoogle.com/
   - Best visual introduction

### 18.5 Books

1. **"Federated Learning: Privacy and Incentive"** by Yang et al. (2020)
   - Comprehensive textbook

2. **"Hands-On Machine Learning"** by GÃ©ron (2022)
   - Chapter on production ML

3. **"Trustworthy Machine Learning"** by Kush Varshney (2022)
   - Covers fairness, robustness, calibration

### 18.6 Communities & Conferences

**Conferences:**
- NeurIPS (Workshop on Federated Learning)
- ICML (Socially Responsible ML)
- ICLR (Privacy in ML)
- ACM CCS (Security & Privacy)

**Communities:**
- OpenMined: https://openmined.org (privacy-preserving ML)
- FL Discord: https://discord.gg/federated-learning
- Reddit: r/MachineLearning, r/privacy

---

## Appendix: Quick Reference

### Key Numbers to Remember

**Performance:**
- Central NN: 81.3% (no privacy)
- FL-LSTM: 79.1% (full privacy)
- Gap: 2.2 percentage points
- Local NN: 56.6% (no collaboration)

**Calibration:**
- FL-MLP before: ECE = 0.227
- FL-MLP after (Beta): ECE = 0.056
- Improvement: 75.3%

**System:**
- 5 clients, 15 rounds
- 16,800 training samples
- 30K+ model parameters
- 13 minutes total runtime

**Heterogeneity:**
- Client 0: 95.6% one class
- Client 2: 6.7% one class (largest)
- Std dev: 0.4189

### Command Cheat Sheet

```bash
# Full experiment with tuning
python main.py --tune --tune-trials 10

# Quick run (no tuning)
python main.py --no-shap --no-lime

# Load and use trained model
python
>>> from load_model import load_model_from_experiment
>>> model, config, scaler = load_model_from_experiment(
...     'fl_experiment_20260111_133448', 'mlp')
>>> predictions = model(X_new)
```

---

**END OF PRESENTATION GUIDE**

*Last updated: January 11, 2026*
*Version: 1.0*
*For questions or contributions, contact: [your-email]*
